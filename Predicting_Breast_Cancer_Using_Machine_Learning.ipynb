{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "949fb61c",
   "metadata": {},
   "source": [
    "## Using Machine Learning to Predict Breast Cancer\n",
    "\n",
    "This project utilizes the __[Breast Cancer Wisconsin (Diagnostic) Data Set](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29)__ for predictive analysis in breast cancer. Key tools used for this project include: Jupyter Notebook, Python - numpy, pandas, matplotlib, plotly, seaborn and scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaca97c",
   "metadata": {},
   "source": [
    "### __[Summary on the Data Set](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29)__\n",
    "\n",
    "Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. A few of the images can be found at [Web Link] \n",
    "\n",
    "Separating plane described above was obtained using Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree Construction Via Linear Programming.\" Proceedings of the 4th Midwest Artificial Intelligence and Cognitive Science Society, pp. 97-101, 1992], a classification method which uses linear programming to construct a decision tree. Relevant features were selected using an exhaustive search in the space of 1-4 features and 1-3 separating planes. \n",
    "\n",
    "The actual linear program used to obtain the separating plane in the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34]. \n",
    "\n",
    "This database is also available through the UW CS ftp server: \n",
    "ftp ftp.cs.wisc.edu \n",
    "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
    "\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "1) ID number \n",
    "2) Diagnosis (M = malignant, B = benign) \n",
    "3-32) \n",
    "\n",
    "Ten real-valued features are computed for each cell nucleus: \n",
    "\n",
    "a) radius (mean of distances from center to points on the perimeter) \n",
    "b) texture (standard deviation of gray-scale values) \n",
    "c) perimeter \n",
    "d) area \n",
    "e) smoothness (local variation in radius lengths) \n",
    "f) compactness (perimeter^2 / area - 1.0) \n",
    "g) concavity (severity of concave portions of the contour) \n",
    "h) concave points (number of concave portions of the contour) \n",
    "i) symmetry \n",
    "j) fractal dimension (\"coastline approximation\" - 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b453a6c9",
   "metadata": {},
   "source": [
    "### Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7437b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.gridspec as gridspec \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import seaborn as sns\n",
    "import mpld3 as mpl\n",
    "import itertools\n",
    "from itertools import chain\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, roc_curve, precision_recall_curve, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, learning_curve, train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as py\n",
    "import plotly.tools as tls\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6b65a9",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daf9184",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Project_Data/WISC_breast_cancer_data.csv\", header = 0)#Loading CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fa086a",
   "metadata": {},
   "source": [
    "### Data Cleaning and Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af7c316",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389c0aa3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962135bf",
   "metadata": {},
   "source": [
    "All columns have the same number of features but for \"Unnamed: 32\". In the next check I will check for and validate missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676cf459",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "null_feat = pd.DataFrame(len(data['id']) - data.isnull().sum(), columns = ['Count'])\n",
    "\n",
    "trace = go.Bar(x = null_feat.index, y = null_feat['Count'] ,opacity = 0.8, marker=dict(color = 'steelblue',\n",
    "        line=dict(color='black',width=1.5)))\n",
    "\n",
    "layout = dict(title =  \"Checking Data Missingness\", plot_bgcolor = \"white\")\n",
    "                    \n",
    "fig = dict(data = [trace], layout=layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acd0c5d",
   "metadata": {},
   "source": [
    "As we can see from the plot above, all features are complete but for 'Unnamed: 32' which has none and therefore will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ea217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping 'Unnamed: 32' with no values\n",
    "data.drop('Unnamed: 32', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ebd8f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9045b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validating drop\n",
    "null_feat = pd.DataFrame(len(data['id']) - data.isnull().sum(), columns = ['Count'])\n",
    "\n",
    "trace = go.Bar(x = null_feat.index, y = null_feat['Count'] ,opacity = 0.8, marker=dict(color = 'steelblue',\n",
    "        line=dict(color='black',width=1.5)))\n",
    "\n",
    "layout = dict(title =  \"Checking Data Missingness\", plot_bgcolor = \"white\")\n",
    "                    \n",
    "fig = dict(data = [trace], layout=layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97df6954",
   "metadata": {},
   "source": [
    "Feature 'Unnamed: 32' has been dropped from the dataset\n",
    "\n",
    "\n",
    "As we can see from the dataset, we have 2 features that represente attribute Information: 1) ID number and 2) Diagnosis (M = malignant, B = benign) \n",
    " \n",
    "\n",
    "The resta are real-valued features (10) that are computed for each cell nucleus: \n",
    "\n",
    "a) radius (mean of distances from center to points on the perimeter) \n",
    "b) texture (standard deviation of gray-scale values) \n",
    "c) perimeter \n",
    "d) area \n",
    "e) smoothness (local variation in radius lengths) \n",
    "f) compactness (perimeter^2 / area - 1.0) \n",
    "g) concavity (severity of concave portions of the contour) \n",
    "h) concave points (number of concave portions of the contour) \n",
    "i) symmetry \n",
    "j) fractal dimension (\"coastline approximation\" - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c920da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb3d366",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.diagnosis.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae5dbcc",
   "metadata": {},
   "source": [
    "### Checking diagnosis distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4aca17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2 datasets\n",
    "# Reassign target\n",
    "data.diagnosis.replace(to_replace = dict(M = 1, B = 0), inplace = True)\n",
    "Malignant = data[(data['diagnosis'] != 0)]\n",
    "Benign = data[(data['diagnosis'] == 0)]\n",
    "\n",
    "trace = go.Bar( x = ['Malignant', 'Benign'], y = (len(Malignant), len(Benign)),opacity = 0.5, marker=dict(color = 'steelblue',\n",
    "        line=dict(color='gray',width=0.5)))\n",
    "\n",
    "layout = dict(title =  'Diagnosis Distribution', plot_bgcolor = \"white\")\n",
    "                    \n",
    "fig = dict(data = [trace], layout=layout)\n",
    "py.iplot(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b39298",
   "metadata": {},
   "source": [
    "### Investigating computed cell nucleus features per diagnosis type: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b20a2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_mean=list(data.columns[1:11])\n",
    "# split dataframe into two based on diagnosis\n",
    "#dfM=df[df['diagnosis'] ==1]\n",
    "#dfB=df[df['diagnosis'] ==0]\n",
    "\n",
    "#Stack the data\n",
    "plt.rcParams.update({'font.size': 8})\n",
    "fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(8,10))\n",
    "axes = axes.ravel()\n",
    "for idx,ax in enumerate(axes):\n",
    "    ax.figure\n",
    "    binwidth= (max(data[features_mean[idx]]) - min(data[features_mean[idx]]))/50\n",
    "    ax.hist([Malignant[features_mean[idx]],Benign[features_mean[idx]]], bins=np.arange(min(data[features_mean[idx]]), max(data[features_mean[idx]]) + binwidth, binwidth) , alpha=0.5,stacked=True, density=True, label=['Malignant','Benign'],color=['steelblue','darkorange'])\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.set_title(features_mean[idx])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ec0379",
   "metadata": {},
   "source": [
    "Overall, we consistently see higher mean values per feature in malignant cells. We can leverage this for our classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00195c0",
   "metadata": {},
   "source": [
    "### Checking Feature Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b49b8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = data.corr()\n",
    "plt.figure(figsize=(20,20))\n",
    "sns.heatmap(data.corr(),  annot = True, cmap=\"cividis\")\n",
    "plt.title(\"Correlation Plot\", fontweight = \"bold\", fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b00c17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Pair-wise comparison of features\n",
    "sns.pairplot(data[corr], diag_kind = \"kde\", markers = \"+\", hue = \"diagnosis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44897885",
   "metadata": {},
   "source": [
    "Based on the above analyses and observations, we can leverage correlated features and reasonably hypothesize that the cancer diagnosis depends on these."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a0e49f",
   "metadata": {},
   "source": [
    "### Creating test and training datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2eb0be",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "traindf, testdf = train_test_split(data, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0040675d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generic function for making a classification model and accessing the performance. \n",
    "# From AnalyticsVidhya tutorial\n",
    "def classification_model(model, data, predictors, outcome):\n",
    "  #Fit the model:\n",
    "  model.fit(data[predictors],data[outcome])\n",
    "  \n",
    "  #Make predictions on training set:\n",
    "  predictions = model.predict(data[predictors])\n",
    "  \n",
    "  #Print accuracy\n",
    "  accuracy = metrics.accuracy_score(predictions,data[outcome])\n",
    "  print(\"Accuracy : %s\" % \"{0:.3%}\".format(accuracy))\n",
    "\n",
    "  #Perform k-fold cross-validation with 5 folds\n",
    "  #kf = KFold(data.shape[0], n_folds=5)\n",
    "\n",
    "  kf = KFold(n_splits = 5)\n",
    "\n",
    "  error = []\n",
    "  for train, test in kf.split(data[predictors]):\n",
    "    # Filter training data\n",
    "    train_predictors = (data[predictors].iloc[train,:])\n",
    "    \n",
    "    # The target we're using to train the algorithm.\n",
    "    train_target = data[outcome].iloc[train]\n",
    "    \n",
    "    # Training the algorithm using the predictors and target.\n",
    "    model.fit(train_predictors, train_target)\n",
    "    \n",
    "    #Record error from each cross-validation run\n",
    "    error.append(model.score(data[predictors].iloc[test,:], data[outcome].iloc[test]))\n",
    "    \n",
    "    print(\"Cross-Validation Score : %s\" % \"{0:.3%}\".format(np.mean(error)))\n",
    "    \n",
    "  #Fit the model again so that it can be refered outside the function:\n",
    "  model.fit(data[predictors],data[outcome]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96977c63",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb509f0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictor_var = ['radius_mean','perimeter_mean','area_mean','compactness_mean','concave points_mean']\n",
    "outcome_var='diagnosis'\n",
    "model=LogisticRegression()\n",
    "classification_model(model,traindf,predictor_var,outcome_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cae7506",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_var = ['radius_mean']\n",
    "outcome_var='diagnosis'\n",
    "model=LogisticRegression()\n",
    "classification_model(model,traindf,predictor_var,outcome_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9155baf",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d32e2e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use all the features of the nucleus\n",
    "predictor_var = features_mean\n",
    "model = RandomForestClassifier(n_estimators=100,min_samples_split=25, max_depth=7, max_features=2)\n",
    "classification_model(model, traindf,predictor_var,outcome_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cf49ff",
   "metadata": {},
   "source": [
    "Using all the features improves the prediction accuracy and the cross-validation score is great.\n",
    "An advantage with Random Forest is that it returns a feature importance matrix which can be used to select features. So lets select the top 5 features and use them as predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b52f24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a series with feature importances:\n",
    "featimp = pd.Series(model.feature_importances_, index=predictor_var).sort_values(ascending=False)\n",
    "print(featimp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe857c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using top 5 features\n",
    "predictor_var = ['concave points_mean','area_mean','radius_mean','perimeter_mean','concavity_mean',]\n",
    "model = RandomForestClassifier(n_estimators=100, min_samples_split=25, max_depth=7, max_features=2)\n",
    "classification_model(model,traindf,predictor_var,outcome_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2645bf",
   "metadata": {},
   "source": [
    "Using the top 5 features only changes the prediction accuracy a bit but I think we get a better result if we use all the predictors.\n",
    "What happens if we use a single predictor as before? Just check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b990c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_var =  ['radius_mean']\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "classification_model(model, traindf,predictor_var,outcome_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fd6ae0",
   "metadata": {},
   "source": [
    "This gives a better prediction accuracy too but the cross-validation is not great. Below I will assess other classifiers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7334f80",
   "metadata": {},
   "source": [
    "### KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af03ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_var = ['concave points_mean','area_mean','radius_mean','perimeter_mean','concavity_mean',]\n",
    "model = KNeighborsClassifier(n_neighbors = 2, weights ='uniform')\n",
    "classification_model(model,traindf,predictor_var,outcome_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea293f9",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed28ed1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model =SVC(kernel=\"rbf\",random_state=15)\n",
    "classification_model(model, traindf,predictor_var,outcome_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af76598",
   "metadata": {},
   "source": [
    "### DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d401da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_var = ['concave points_mean','area_mean','radius_mean','perimeter_mean','concavity_mean',]\n",
    "model=DecisionTreeClassifier(random_state=10)\n",
    "classification_model(model, traindf,predictor_var,outcome_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53671627",
   "metadata": {},
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ecd39d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predictor_var = ['concave points_mean','area_mean','radius_mean','perimeter_mean','concavity_mean',]\n",
    "#model=RandomForestClassifier(n_estimators=60, random_state=0)\n",
    "model=RandomForestClassifier(n_estimators=100,min_samples_split=25, max_depth=7, max_features=2)\n",
    "classification_model(model, traindf,predictor_var,outcome_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54df4f9d",
   "metadata": {},
   "source": [
    "### GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f059cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_var = ['concave points_mean','area_mean','radius_mean','perimeter_mean','concavity_mean',]\n",
    "model=GradientBoostingClassifier(random_state=20)\n",
    "classification_model(model, traindf,predictor_var,outcome_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe410789",
   "metadata": {},
   "source": [
    "### AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1f37b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_var = ['concave points_mean','area_mean','radius_mean','perimeter_mean','concavity_mean',]\n",
    "model=AdaBoostClassifier()\n",
    "classification_model(model, traindf,predictor_var,outcome_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0803d61",
   "metadata": {},
   "source": [
    "### XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e49a061",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictor_var = ['concave points_mean','area_mean','radius_mean','perimeter_mean','concavity_mean',]\n",
    "model=xgb.XGBClassifier(random_state=0,booster=\"gbtree\", eval_metric=\"logloss\")\n",
    "classification_model(model, traindf,predictor_var,outcome_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a72e9c",
   "metadata": {},
   "source": [
    "## Using on the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0852fbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use all the features of the nucleus\n",
    "predictor_var = features_mean\n",
    "model = RandomForestClassifier(n_estimators=100,min_samples_split=25, max_depth=7, max_features=2)\n",
    "classification_model(model, testdf,predictor_var,outcome_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855f1d97",
   "metadata": {},
   "source": [
    "### Model Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f60eb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "models=['Logistic Regression ', 'Random Forest', 'KNeighbors','SVC', 'DecisionTree', 'GradientBoosting','AdaBoost', 'XGB']\n",
    "\n",
    "plot = go.Bar(x=models, y=[89.698, 95.477, 90.201,88.945,100.000, 100.000, 98.492,100.000],\n",
    "               opacity = 0.5, marker=dict(color = 'steelblue',line=dict(color='gray',width=0.5))) \n",
    "\n",
    "layout = dict(title =  'Model Accuracies', plot_bgcolor = \"white\")\n",
    "                    \n",
    "fig = dict(data = [plot], layout=layout)\n",
    "py.iplot(fig)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
